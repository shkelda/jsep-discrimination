{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Annex 2: Data scrapping for comrehensive case analysis\n",
    "<div class=\"alert alert-block alert-danger\"><b>Info:</b> This page is being developed.</div>\n",
    "This notebook describes the process of identifying, downloading, extracting and cleaning data for case files published on http://act.sot.kg. This is a technical step for a comprehensive case analysis of judicial decisions in relation to discrimination in Kyrgyzstan undertaken by the UN Human Rights Office for Central Asia.\n",
    "\n",
    "The notebook contains all neccessary Python (3.8) code to execute the entire process independently. For further information please contact Peter Naderer (peter.naderer@outlook.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we want to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scrapping the website and identifying data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def souping(court, act_type):\n",
    "\n",
    "    '''\n",
    "    :court Type of court\n",
    "    :act_type Type of the document to be downloaded\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    url_list = []\n",
    "    link_list = []\n",
    "    data = []\n",
    "\n",
    "    url = str('http://act.sot.kg/ru/search?caseno=&name=&articles=&court={}&judge=all&caseOpenedFrom=&caseType=all&actType={}&caseOpenedTo=&from=&to=&side1=&side2=&submit-act=%D0%90%D0%BA%D1%82%D1%8B&quantity=5000&page='.format(court, act_type))\n",
    "               # SET QUANTITY TO APPROPRIATE LEVEL\n",
    "\n",
    "    print('Finding out how many pages I need to look through.')\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "    lastpage = soup.find(\"li\", class_=\"last\")\n",
    "    lastpage = lastpage.find('a')['href']\n",
    "    lastpage = int(re.search('\\d*$', lastpage).group(0))\n",
    "    lastpage += 1\n",
    "    page_list = list(range(1, lastpage))  # For testing only, ENDVALUE should be lastpage\n",
    "\n",
    "    print('I going to download information from {} pages'.format(lastpage))\n",
    "\n",
    "    for i in page_list:\n",
    "        url_list.append(url + str(i))\n",
    "\n",
    "    for counter, url in enumerate(url_list, start=1):\n",
    "        print('I am working on page {} of {}'.format(counter, lastpage-1))\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        table = soup.find('table')\n",
    "        table_body = table.find('tbody')\n",
    "        rows = table_body.find_all('tr')\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            cols_strip = []\n",
    "            element = []\n",
    "            links = []\n",
    "            for ele in cols:\n",
    "                cols_strip.append(ele.text.strip())\n",
    "            for i in cols_strip:\n",
    "                element.append(i)\n",
    "            for ele in cols:\n",
    "                link = ele.find('a')\n",
    "                if link != None:\n",
    "                    link = link.get('href')\n",
    "                    links.append(link)\n",
    "                    links_list = links\n",
    "            element = element + links_list\n",
    "            data.append(element)\n",
    "\n",
    "    return data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    court = \"\"\n",
    "    act_type = '2' # Постановление = 4 // Приговор = 2\n",
    "\n",
    "    result = souping(court, act_type)\n",
    "\n",
    "    with open(r'..\\data\\sotkg_data_acttype_{}_{}.pkl'.format(act_type, date.today()), 'wb') as file:\n",
    "        pickle.dump(result, file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Downloading identified files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import os\n",
    "import re\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import concurrent.futures\n",
    "from functools import wraps\n",
    "from tqdm import tqdm\n",
    "\n",
    "def timeit(method):\n",
    "    @wraps(method)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{method.__name__} => {(end_time-start_time)*1000} ms\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "@timeit\n",
    "def downloading_one(url, directory):\n",
    "\n",
    "    req = urllib.request.urlopen(url)\n",
    "    fullpath = Path(url)\n",
    "    fname = fullpath.name\n",
    "\n",
    "        # Combine the name and the downloads directory to get the local filename\n",
    "    download = os.path.join(directory, fname)\n",
    "\n",
    "    # print('I am now attempting to download {}'.format(fname))\n",
    "    if not os.path.isfile(download): # Do not download the file if it already exists in directory\n",
    "        try:\n",
    "            urlretrieve(url, download)\n",
    "        except urllib.error.HTTPError as e:\n",
    "            print(e.reason)\n",
    "\n",
    "@timeit\n",
    "def downloading_bulk_singleprocess(url_list):\n",
    "    print('I will download {} files.'.format(len(url_list)))\n",
    "    return [downloading_one(url, directory) for url in url_list]\n",
    "\n",
    "@timeit\n",
    "def downloading_bulk(url_list):\n",
    "    '''\n",
    "    This function uses download_one() to download a list of files with multi-threading.\n",
    "    '''\n",
    "    with tqdm(total=int(len(url_list))) as pbar:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            futures = {executor.submit(downloading_one, url, directory): url for url in url_list}\n",
    "            results = {}\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                arg = futures[future]\n",
    "                results[arg] = future.result()\n",
    "                pbar.update(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # url = 'http://act.sot.kg/act/download/190580.pdf'\n",
    "    # url_list = ['http://act.sot.kg/act/download/68.pdf','http://act.sot.kg/act/download/70.pdf', 'http://act.sot.kg/act/download/83.pdf','http://act.sot.kg/act/download/94.pdf']\n",
    "    directory = r'C:\\Users\\peter\\Python\\research_files\\files'\n",
    "\n",
    "    df = pd.read_pickle(r'../data/DF_acttype_4_2020-11-01.pkl')\n",
    "    url_list = df['link_file'].tolist()\n",
    "\n",
    "    downloading_bulk(url_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extracting text from case files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
